# Using float16 on supported devices

In this tutorial we'll walk through how once can make use of float16 support which is now supported by the latest Volta range of NVidia GPUs.
The float16 data type also known as half precision reduces memory usage of the model, allowing the training and deployment of larger models.
Besides this, they would also reduce the time for transfer of data when compared to using float32 (single precision) or float64 (double precision).

## Prerequisites

Using float16 does not necessarily lead to faster performance, unless the underlying hardware has native support for the data type and
the workload is large enough to benefit from the optimizations. Examples of such hardware are the new Volta range of Graphics Processing Units by Nvidia.
For the rest of this tutorial we assume we are using such Nvidia GPUs.

- Volta or newer range of Nvidia GPUs
- Cuda 9 or higher
- CUDNN v7 or higher


## Using the Symbol API

### Training

MXNet's layers can generally work with any data type.
Operators infer the data type from input data and perform computation with that data type.
So to enable training or inference in float16, all we need to do is add a Cast layer before the first layer.
In this section, let us walk through the changes required to allow training with float16 precision.
We will be training a VGG11 network with Imagenet shaped dummy data in this example.
The complete example is in [train_imagenet.py](example/image-classification/train_imagenet.py) and the network is defined in [vgg.py](example/image-classification/symbols/vgg.py).

Firstly, we need to ensure that the data received by the network is of type float16.
If the data iterator itself is producing float16 data, then we are set.
If not, we can add a cast layer as the first layer of the network.

```python

def get_feature(internel_layer, layers, filters, batch_norm = False, **kwargs):
    for i, num in enumerate(layers):
        for j in range(num):
            internel_layer = mx.sym.Convolution(data = internel_layer, kernel=(3, 3), pad=(1, 1), num_filter=filters[i], name="conv%s_%s" %(i + 1, j + 1))
            if batch_norm:
                internel_layer = mx.symbol.BatchNorm(data=internel_layer, name="bn%s_%s" %(i + 1, j + 1))
            internel_layer = mx.sym.Activation(data=internel_layer, act_type="relu", name="relu%s_%s" %(i + 1, j + 1))
        internel_layer = mx.sym.Pooling(data=internel_layer, pool_type="max", kernel=(2, 2), stride=(2,2), name="pool%s" %(i + 1))
    return internel_layer

def get_classifier(input_data, num_classes, **kwargs):
    flatten = mx.sym.Flatten(data=input_data, name="flatten")
    fc6 = mx.sym.FullyConnected(data=flatten, num_hidden=4096, name="fc6")
    relu6 = mx.sym.Activation(data=fc6, act_type="relu", name="relu6")
    drop6 = mx.sym.Dropout(data=relu6, p=0.5, name="drop6")
    fc7 = mx.sym.FullyConnected(data=drop6, num_hidden=4096, name="fc7")
    relu7 = mx.sym.Activation(data=fc7, act_type="relu", name="relu7")
    drop7 = mx.sym.Dropout(data=relu7, p=0.5, name="drop7")
    fc8 = mx.sym.FullyConnected(data=drop7, num_hidden=num_classes, name="fc8")
    return fc8

# vgg11 spec
layers, filters = ([1, 1, 2, 2, 2], [64, 128, 256, 512, 512])

data = mx.sym.Variable(name="data")
if dtype == 'float16':
    data = mx.sym.Cast(data=data, dtype=np.float16)
feature = get_feature(data, layers, filters, batch_norm)
classifier = get_classifier(feature, num_classes)
if dtype == 'float16':
    out = mx.sym.Cast(data=out, dtype=np.float32)
symbol = mx.sym.SoftmaxOutput(data=out, name='softmax')

```


### Fine tuning a model trained in float32

This requires the pre-trained symbol to support fp16. If we load a symbol trained with float32, then it would continue to use

## Using Gluon API

### Training

We need to take care of two things to convert a model to support float16.
Gluon Blocks have a cast method which casts parameters and changes the types of input expected.
This is not all though, we still need to ensure that data input to the block is in the form of float16.
This can be done by casting the data to float16. If the iterator supports generating data in float16 representation, we are set.
Else, we need to cast the data generated by data iterator.
An example of this can be seen in [example/gluon/image_classification.py](example/gluon/image_classification.py).

```python
for i, batch in enumerate(train_data):
    if batch.data[0].dtype != np.dtype(opt.dtype):
        batch.data[0] = batch.data[0].astype(opt.dtype)
    data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
    label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)
    ...
```


### Fine tuning

Looks like we can just cast the block we load. Need to test it


## Things to keep in mind
- MXNET_CUDNN_AUTOTUNE_DEFAULT
- Batch size
- Model size
- SGEMM kernels

