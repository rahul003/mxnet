# Using float16 on supported devices

In this tutorial we'll walk through how once can make use of float16 support which is now supported by the latest Volta range of NVidia GPUs.
The float16 data type also known as half precision reduces memory usage of the model, allowing the training and deployment of larger models.
Besides this, they would also reduce the time for transfer of data when compared to using float32 (single precision) or float64 (double precision).

## Prerequisites

Using float16 does not necessarily lead to faster performance, unless the underlying hardware has native support for the data type and
the workload is large enough to benefit from the optimizations. Examples of such hardware are the new Volta range of Graphics Processing Units by Nvidia.
For the rest of this tutorial we assume we are using such Nvidia GPUs.

- Volta or newer range of Nvidia GPUs
- Cuda 9 or higher
- CUDNN v7 or higher


## Using the Symbol API

### Training

MXNet's layers can generally work with any data type.
Operators infer the data type from input data and perform computation with that data type.
So to enable training or inference in float16, all we need to do is add a Cast layer before the first layer.
In this section, let us walk through the changes required to allow training with float16 precision.
We will be training a VGG11 network with Imagenet shaped dummy data in this example.
The complete example is in [train_imagenet.py](example/image-classification/train_imagenet.py) and the network is defined in [vgg.py](example/image-classification/symbols/vgg.py).

Firstly, we need to ensure that the data received by the network is of type float16.
If the data iterator itself is producing float16 data, then we are set.
If not, we can add a cast layer as the first layer of the network.

But softmax requires fp32, so cast it back at the end.

### Fine tuning a model trained in float32

This requires the pre-trained symbol to support fp16.
If we load a symbol trained with float32, then it would continue to expect float32.
We need to create the same model as used to train the model, and add cast layers to use float16 input.

## Using Gluon API

### Training

We need to take care of two things to convert a model to support float16.
Gluon Blocks have a cast method which casts parameters and changes the types of input expected.
This is not all though, we still need to ensure that data input to the block is in the form of float16.
This can be done by casting the data to float16. If the iterator supports generating data in float16 representation, we are set.
Else, we need to cast the data generated by data iterator.
An example of this can be seen in [example/gluon/image_classification.py](example/gluon/image_classification.py).

```python
for i, batch in enumerate(train_data):
    if batch.data[0].dtype != np.dtype(opt.dtype):
        batch.data[0] = batch.data[0].astype(opt.dtype)
    data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
    label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)
    ...
```


### Fine tuning

Looks like we can just cast the block we load. Need to test it. Segfaults.


## Things to keep in mind
- Setting the environment variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 2 can help run tests to pick the fastest convolution algorithm at runtime.
Refer [Environment variables in MXNet](env_var.md) for more details.
- Batch size: It's recommended to use batch sizes which are multiples of 8, as Nvidia's Tensor Cores perform best when dimensions of inputs are multiples of 8.
- Model size: For smaller models like when training Resnet50 for Cifar10, most matrices involved in the computation can not benefit from Tensor cores.
Training on a single GPU with float16 in such a case can thus even be slower than training with float32.
- When training using multiple GPUs, reduced communication times with float16 also contribute to improved performance.
- You can check whether your program is using Tensor cores for fast float16 computation by profiling with `nvprof`.
Operations with `s884cudnn` in their names represent the use of Tensor cores.


